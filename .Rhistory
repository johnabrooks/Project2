translationFeed[i],
target = "en",
format = "text",
source = "fr",
model = "nmt"
)
}
translationFrame <- rbind(translationFrame, translationEnFr)
}
getwd()
?saveRDS()
saveRDS(translationFrame,"frenchToEnglish.rds")
g <- readRDS("frenchToEnglish.rds")
g
write.xlsx(g, "frenchToEnglish.xlsx")
View(cleanData)
# Extra
d <- "off toff staffed on the cuff ff (fford's), AoS AA"
str_replace_all(d,"\\<[[:upper:]]*[[:upper:]]*\\>","")
library(xlsx)
library(tidyr)
library(tidytext)
library(tidyverse)
library(dplyr)
library(stringr)
str_replace_all(d,"\\<[[:upper:]]*[[:upper:]]*\\>","")
str_replace_all(d,"\\<[[:upper:]][[:upper:]]\\>","")
str_replace_all(d,"\\b[[:upper:]][[:upper:]]\\b","")
str_replace_all(d,"\\b[[:upper:]].[[:upper:]].\\b","")
initialismPattern <- c(
"\\b[[:upper:]]\\w[[:upper:]].\\b"
)
str_replace_all(d,initialismPattern,"")
initialismPattern <- c(
"\\b[[:upper:]]\\w[[:upper:]]\\w\\b",
"\\b[[:upper:]][[:upper:]]\\w\\b",
"\\b[[:upper:]][[:upper:]]\\b"
)
initialismPattern
# Initialize libraries
## Standard
library(xlsx)
library(tidyr)
library(tidytext)
library(tidyverse)
library(dplyr)
library(stringr)
## Words list that includes variations of words (e.g. searched, searching, etc. for search)
# install.packages("qdapDictionaries")
library(qdapDictionaries)
## Translation
# install.packages("googleLanguageR")
library(googleLanguageR)
# Load authorization
gl_auth("/Users/johnbrooks/Dropbox/Synced/Credentials/API/STAT 5702 - Text Translation-df0390ca10f9.json")
# https://cran.r-project.org/web/packages/googleLanguageR/vignettes/setup.html
# Read in file
dirIn <- "/Users/johnbrooks/Desktop/Course Work/STAT5702/Project2"
fileIn <- "Redacted FAB_Project_raw_data_Clean EXCEL Dec.23"
# Create Path
pathIn <- paste(dirIn,"/",fileIn,".xlsx",sep="")
# Read in data
rawData <- read.xlsx(pathIn,1)
# Process columns
columnNamesStore <- names(rawData)
cleanData <- rawData
names(cleanData) <- paste("c",1:ncol(cleanData),sep="_")
View(cleanData)
nonCatColumns <- c(14,
22,
30,
40,
41,
43,
45,
46,
48)
# Scrape free text columns
procData <- cleanData[,c(1,2,nonCatColumns)]
newNames <- names(procData)
selectNames  <- newNames[c(3:length(newNames))]
######### Translate here
# Pivot the data
pivtData <- procData %>%
# Select only those columns with the ID and the phrases
select(c("c_1","c_2",selectNames)) %>%
# Pivot the data to be cataloged by ID and question index
pivot_longer(selectNames,names_to = "column",values_to = "response")
# Prepare data for translation
forTranslation <- pivtData %>%
# Take out french
filter(c_2 == "FR")
# Count Characters
counterChar <- 0
for(currentStr in forTranslation$response) {
counterChar <- counterChar + str_length(currentStr)
}
# Translational functional block
## Commented out as running it too many time could mean $$$
## control + shift + C to activate / deactivate lines
# translationFeed <- forTranslation$response[!(forTranslation$response == "")]
#
# translationFrame <- gl_translate(
#   t_string,
#   target = "en",
#   format = "text",
#   source = "fr",
#   model = "nmt"
# )
#
# for (i in 2:length(translationFeed)){
#   if(i!=8){
#     translationEnFr <- gl_translate(
#       translationFeed[i],
#       target = "en",
#       format = "text",
#       source = "fr",
#       model = "nmt"
#     )
#   }
#   translationFrame <- rbind(translationFrame, translationEnFr)
# }
#
# saveRDS(translationFrame,"frenchToEnglish.rds")
# How to read / write the file
# g <- readRDS("frenchToEnglish.rds")
# write.xlsx(g, "frenchToEnglish.xlsx")
# Prepare data for assessment
forProcessEng <- pivtData %>%
# Take out french
filter(c_2 == "EN") %>%
# Get the responses into words
unnest_tokens(word, response) %>%
# Select the words column
select(c("word")) %>%
# Get the unique words
unique()
grepl(forProcessEng,initialismPattern)
forProcessEng
View(pivtData)
grep(pivtData$response,initialismPattern)
str_extract_all(pivtData$response,initialismPattern)
str_extract_all(pivtData$response,initialismPattern[1])
responsesTogether <- paste(pivtData$response, collapse = "\n")
View(responsesTogether)
responsesTogether
str_extract_all(responsesTogether,initialismPattern[1])
str_extract_all(responsesTogether,initialismPattern)
listInitialisms <- unlist(str_extract_all(responsesTogether,initialismPattern))
listInitialisms
listInitialisms <- unlist(str_extract_all(responsesTogether,initialismPattern)) %>%
unique()
listInitialisms
initialismPattern
# Create patterns to find initialisms
initialismPattern <- c(
"\\b\\w[[:upper:]]\\w[[:upper:]]\\w\\b",
"\\b[[:upper:]]\\w[[:upper:]]\\w\\b",
"\\b\\w[[:upper:]]\\w[[:upper:]]\\b",
"\\b\\w[[:upper:]][[:upper:]]\\w\\b",
"\\b[[:upper:]][[:upper:]]\\w\\b",
"\\b\\w[[:upper:]][[:upper:]]\\b",
"\\b[[:upper:]]\\w[[:upper:]]\\b",
"\\b[[:upper:]][[:upper:]]\\b"
)
responsesTogether <- paste(pivtData$response, collapse = "\n")
listInitialisms <- unlist(str_extract_all(responsesTogether,initialismPattern)) %>%
unique()
listInitialisms
# Create patterns to find initialisms
initialismPattern <- c(
"\\b\\w+[[:upper:]]\\w+[[:upper:]]\\w+\\b",
"\\b[[:upper:]]\\w+[[:upper:]]\\w+\\b",
"\\b\\w+[[:upper:]]\\w+[[:upper:]]\\b",
"\\b\\w+[[:upper:]][[:upper:]]\\w+\\b",
"\\b[[:upper:]][[:upper:]]\\w+\\b",
"\\b\\w+[[:upper:]][[:upper:]]\\b",
"\\b[[:upper:]]\\w+[[:upper:]]\\b",
"\\b[[:upper:]][[:upper:]]\\b"
)
listInitialisms <- unlist(str_extract_all(responsesTogether,initialismPattern)) %>%
unique()
listInitialisms
# Create patterns to find initialisms
initialismPattern <- c(
# Natural strings
"\\b\\w+[[:upper:]]\\w+[[:upper:]]\\w+\\b",
"\\b[[:upper:]]\\w+[[:upper:]]\\w+\\b",
"\\b\\w+[[:upper:]]\\w+[[:upper:]]\\b",
"\\b\\w+[[:upper:]][[:upper:]]\\w+\\b",
"\\b[[:upper:]][[:upper:]]\\w+\\b",
"\\b\\w+[[:upper:]][[:upper:]]\\b",
"\\b[[:upper:]]\\w+[[:upper:]]\\b",
"\\b[[:upper:]][[:upper:]]\\b",
# Possessive strings
"\\b\\w+[[:upper:]]\\w+[[:upper:]]\\w+\'s\\b",
"\\b[[:upper:]]\\w+[[:upper:]]\\w+\'s\\b",
"\\b\\w+[[:upper:]]\\w+[[:upper:]]\'s\\b",
"\\b\\w+[[:upper:]][[:upper:]]\\w+\'s\\b",
"\\b[[:upper:]][[:upper:]]\\w+\'s\\b",
"\\b\\w+[[:upper:]][[:upper:]]\'s\\b",
"\\b[[:upper:]]\\w+[[:upper:]]\'s\\b",
"\\b[[:upper:]][[:upper:]]\'s\\b"
)
listInitialisms <- unlist(str_extract_all(responsesTogether,initialismPattern)) %>%
unique()
listInitialisms
sort(listInitialisms)
listInitialisms <- unlist(str_extract_all(responsesTogether,initialismPattern)) %>%
unique() %>%
sort()
listInitialisms
?grep()
# For each initialism find where it was discovered
grep(listInitialisms[1],pivtData$response)
Length(pivtData$response)
pivtData$response
length(pivtData$response)
# For each initialism find where it was discovered
indexHold <- c()
respondsHold <- c()
for(initialismIndex in 1:length(listInitialisms)){
currentResponses <- grep(listInitialisms[initialismIndex],pivtData$response)
respondsHold <- c(respondsHold, currentResponses)
indexHold <- c(indexHold, rep(initialismIndex,length(currentResponses)))
}
respondsHold
# Decode verifiction frame
verificationFrame <- data.frame(listInitialisms[indexHold],
pivtData$response[respondsHold])
# Write the verification fram to an excel file for ease of viewing
write.xlsx(verificationFrame,"initialsVerification.xlsx")
pathToOfflineFiles <- "/Users/johnbrooks/Dropbox/R_files/Users/johnbrooks/Dropbox/Synced/R/STAT 5702/Store"
pathToOfflineFiles
paste(pathToOfflineFiles,"a",sep="/")
translationFrame <-
readxl::read_xlsx(paste(pathToOfflineFiles,"frenchToEnglishM.xlsx",sep="/"))
translationFrame
!file.exists(paste(pathToOfflineFiles,"frenchToEnglish.rds",sep="/"))
# Initialize libraries
## Standard
library(xlsx)
library(tidyr)
library(tidytext)
library(tidyverse)
library(dplyr)
library(stringr)
## Words list that includes variations of words (e.g. searched, searching, etc. for search)
# install.packages("qdapDictionaries")
library(qdapDictionaries)
## Translation
# install.packages("googleLanguageR")
library(googleLanguageR)
# Load authorization
gl_auth("/Users/johnbrooks/Dropbox/Synced/Credentials/API/STAT 5702 - Text Translation-df0390ca10f9.json")
# https://cran.r-project.org/web/packages/googleLanguageR/vignettes/setup.html
# Read in file
dirIn <- "/Users/johnbrooks/Desktop/Course Work/STAT5702/Project2"
fileIn <- "Redacted FAB_Project_raw_data_Clean EXCEL Dec.23"
# Create Path
pathIn <- paste(dirIn,"/",fileIn,".xlsx",sep="")
# Read in data
rawData <- read.xlsx(pathIn,1)
# Process columns
columnNamesStore <- names(rawData)
cleanData <- rawData
names(cleanData) <- paste("c",1:ncol(cleanData),sep="_")
View(cleanData)
nonCatColumns <- c(14,
22,
30,
40,
41,
43,
45,
46,
48)
# Scrape free text columns
procData <- cleanData[,c(1,2,nonCatColumns)]
newNames <- names(procData)
selectNames  <- newNames[c(3:length(newNames))]
######### Translate here
# Pivot the data
pivtData <- procData %>%
# Select only those columns with the ID and the phrases
select(c("c_1","c_2",selectNames)) %>%
# Pivot the data to be cataloged by ID and question index
pivot_longer(selectNames,names_to = "column",values_to = "response")
# Prepare data for translation
forTranslation <- pivtData %>%
# Take out french
filter(c_2 == "FR")
# Count Characters
counterChar <- 0
for(currentStr in forTranslation$response) {
counterChar <- counterChar + str_length(currentStr)
}
forTranslation
translationFrame
forTranslation %>%
mutate(translation = translationFrame$translatedText) -> FrenchSegment
forTranslation
View(forTranslation)
!(forTranslation$response == "")
forTranslation[!(forTranslation$response == "")] %>%
mutate(translation = translationFrame$translatedText) -> FrenchSegment
forTranslation[!(forTranslation$response == ""),] %>%
mutate(translation = translationFrame$translatedText) -> FrenchSegment
FrenchSegment
forTranslation[!(forTranslation$response == ""),] %>%
mutate(english = translationFrame$translatedText) -> FrenchSegment
FrenchSegment
forTranslation %>%
filter(response == "") %>%
mutate(english = translationFrame$translatedText) -> FrenchSegment
?filter()
forTranslation
forTranslation %>%
filter(response != "") %>%
mutate(english = translationFrame$translatedText) -> FrenchSegment
FrenchSegmentResponse
forTranslation %>%
filter(response != "") %>%
mutate(english = translationFrame$translatedText) -> FrenchSegmentResponse
FrenchSegmentResponse
forTranslation %>%
filter(response == "") %>%
mutate(english == "") -> FrenchSegmentNonResponse
forTranslation %>%
filter(response == "") %>%
mutate(english = "") -> FrenchSegmentNonResponse
FrenchSegmentNonResponse
pivtData
pivtData %>%
# Take out french
filter(c_2 == "EN") -> EnglishFrame
EnglishFrame
pivtData %>%
# Take out french
filter(c_2 == "EN") %>%
mutate(english = response) -> EnglishFrame
EnglishFrame
pivtData %>%
# Take out french
filter(c_2 == "EN") %>%
mutate(english = response) -> EnglishTibble
# Master Response
MasterResponse <- rbind(
FrenchSegmentResponse,
FrenchSegmentNonResponse,
EnglishTibble
) %>%
arrange(c_1,c_2,column)
MasterResponse
View(MasterResponse)
translationFrame
rm(translationFrame)
# Read in improved matrix
if(file.exists(paste(pathToOfflineFiles,"frenchToEnglishM.xlsx",sep="/"))){
translationFrame <-
readxl::read_xlsx(paste(pathToOfflineFiles,"frenchToEnglishM.xlsx",sep="/"))
}
translationFrame
# Create master translated tibble
if(file.exists(paste(pathToOfflineFiles,"masterResponse.xlsx",sep="/"))){
forTranslation %>%
filter(response != "") %>%
mutate(english = translationFrame$translatedText) -> FrenchSegmentResponse
forTranslation %>%
filter(response == "") %>%
mutate(english = "") -> FrenchSegmentNonResponse
pivtData %>%
# Take out french
filter(c_2 == "EN") %>%
mutate(english = response) -> EnglishTibble
# Master Response
MasterResponse <- rbind(
FrenchSegmentResponse,
FrenchSegmentNonResponse,
EnglishTibble
) %>%
arrange(c_1,c_2,column)
write.xlsx(translationFrame, paste(pathToOfflineFiles,"masterResponse",sep="/"))
} else {
readxl::read_xlsx(paste(pathToOfflineFiles,"masterResponse.xlsx",sep="/"))
}
if(file.exists(paste(pathToOfflineFiles,"masterResponse.xlsx",sep="/")))
)
file.exists(paste(pathToOfflineFiles,"masterResponse.xlsx",sep="/"))
write.xlsx(MasterResponse, paste(pathToOfflineFiles,"masterResponse",sep="/"))
# Initialize libraries
## Standard
library(xlsx)
library(tidyr)
library(tidytext)
library(tidyverse)
library(dplyr)
library(stringr)
## Words list that includes variations of words (e.g. searched, searching, etc. for search)
# install.packages("qdapDictionaries")
library(qdapDictionaries)
## Translation
# install.packages("googleLanguageR")
library(googleLanguageR)
# Load authorization
gl_auth("/Users/johnbrooks/Dropbox/Synced/Credentials/API/STAT 5702 - Text Translation-df0390ca10f9.json")
# https://cran.r-project.org/web/packages/googleLanguageR/vignettes/setup.html
# Read in file
dirIn <- "/Users/johnbrooks/Desktop/Course Work/STAT5702/Project2"
fileIn <- "Redacted FAB_Project_raw_data_Clean EXCEL Dec.23"
# Create Path
pathIn <- paste(dirIn,"/",fileIn,".xlsx",sep="")
# Read in data
rawData <- read.xlsx(pathIn,1)
# Process columns
columnNamesStore <- names(rawData)
cleanData <- rawData
names(cleanData) <- paste("c",1:ncol(cleanData),sep="_")
# If you want to see the data
# View(cleanData)
nonCatColumns <- c(14,
22,
30,
40,
41,
43,
45,
46,
48)
# Scrape free text columns
procData <- cleanData[,c(1,2,nonCatColumns)]
newNames <- names(procData)
selectNames  <- newNames[c(3:length(newNames))]
######### Translate here
# Pivot the data
pivtData <- procData %>%
# Select only those columns with the ID and the phrases
select(c("c_1","c_2",selectNames)) %>%
# Pivot the data to be cataloged by ID and question index
pivot_longer(selectNames,names_to = "column",values_to = "response")
# Prepare data for translation
forTranslation <- pivtData %>%
# Take out french
filter(c_2 == "FR")
# Count Characters
counterChar <- 0
for(currentStr in forTranslation$response) {
counterChar <- counterChar + str_length(currentStr)
}
# Translational functional block
## Commented out as running it too many time could mean $$$
## control + shift + C to activate / deactivate lines
pathToOfflineFiles <- "/Users/johnbrooks/Dropbox/R_files/Users/johnbrooks/Dropbox/Synced/R/STAT 5702/Store"
if(!file.exists(paste(pathToOfflineFiles,"frenchToEnglish.rds",sep="/"))){
translationFeed <- forTranslation$response[!(forTranslation$response == "")]
translationFrame <- gl_translate(
t_string,
target = "en",
format = "text",
source = "fr",
model = "nmt"
)
for (i in 2:length(translationFeed)){
if(i!=8){
translationEnFr <- gl_translate(
translationFeed[i],
target = "en",
format = "text",
source = "fr",
model = "nmt"
)
}
translationFrame <- rbind(translationFrame, translationEnFr)
}
# Save the feed
saveRDS(translationFrame,
paste(pathToOfflineFiles,"frenchToEnglish.rds",sep="/"))
write.xlsx(translationFrame, paste(pathToOfflineFiles,"frenchToEnglish",sep="/"))
} else {
# How to read / write the file: just adjust the path
translationFrame <- readRDS(paste(pathToOfflineFiles,"frenchToEnglish",sep="/"))
}
# Read in improved matrix
if(file.exists(paste(pathToOfflineFiles,"frenchToEnglishM.xlsx",sep="/"))){
translationFrame <-
readxl::read_xlsx(paste(pathToOfflineFiles,"frenchToEnglishM.xlsx",sep="/"))
}
# Create master translated tibble
if(file.exists(paste(pathToOfflineFiles,"masterResponse.xlsx",sep="/"))){
forTranslation %>%
filter(response != "") %>%
mutate(english = translationFrame$translatedText) -> FrenchSegmentResponse
forTranslation %>%
filter(response == "") %>%
mutate(english = "") -> FrenchSegmentNonResponse
pivtData %>%
# Take out french
filter(c_2 == "EN") %>%
mutate(english = response) -> EnglishTibble
# Master Response
MasterResponse <- rbind(
FrenchSegmentResponse,
FrenchSegmentNonResponse,
EnglishTibble
) %>%
arrange(c_1,c_2,column)
write.xlsx(MasterResponse, paste(pathToOfflineFiles,"masterResponse",sep="/"))
} else {
readxl::read_xlsx(paste(pathToOfflineFiles,"masterResponse.xlsx",sep="/"))
}
translationFrame
write.xlsx(MasterResponse, paste(pathToOfflineFiles,"masterResponse.xlsx",sep="/"))
